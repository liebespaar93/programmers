{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pytorch_test",
   "display_name": "pytorch_test",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/igyeong-geun/opt/miniconda3/envs/pytorch_test/lib/python3.9/site-packages/torch/package/_mock_zipreader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:67.)\n  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "    \n",
    "    def backward(ctx, grad_y):\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_input = grad_y.clone()\n",
    "        grad_input[x < 0] = 0 \n",
    "        return grad_input\n",
    "def my_relu(x):\n",
    "    return MyReLU.apply(x)\n",
    "'''\n",
    "def my_relu(x):\n",
    "    return x.clamp(min=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim\n",
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn Defune new Modules\n",
    "import torch\n",
    "\n",
    "class ParallelBlock(torch.nn.Module):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(ParallelBlock, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, D_out)\n",
    "        self.linear2 = torch.nn.Linear(D_in, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.linear1(x)\n",
    "        h2 = self.linear2(x)\n",
    "        return (h1 * h2).clamp(min=0)\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    ParallelBlock(D_in, H),\n",
    "    ParallelBlock(H, H),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "loader = DataLoader(TensorDataset(x, y), batch_size=8)\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
    "for epoch in range(20):\n",
    "    for x_batch, y_batch in loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = torch.nn.functional.mse_loss(y_pred,y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /Users/igyeong-geun/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/igyeong-geun/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /Users/igyeong-geun/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# 모델의 가중치를 다운받아서 자동으로 하는 라이브러리\n",
    "import torch\n",
    "import torchvision\n",
    "alexnet = torchvision.models.alexnet(pretrained= True)\n",
    "vgg16 = torchvision.models.vgg16(pretrained= True)\n",
    "resnet101 = torchvision.models.resnet101(pretrained= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서 보드 \n",
    "#pip install tensorboardx\n",
    "# 이미지화 해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic computation Graphs\n",
    "# 데이터를 버렷다 다시 만들었다 하는거\n",
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Static Computation Graphs\n",
    "# 연산이 끝나도 버리지않고 사용\n",
    "for x_batch, y_batch in loader:\n",
    "    run_graph(graph, x=x_batch, y=y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow(Neural Net) 2.0 이전\n",
    "import tensorflow as tf \n",
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.placeholder(tf.float32, shape=(D, H))\n",
    "w1 = tf.placeholder(tf.float32, shape=(H, D))\n",
    "\n",
    "h = tf.maximum(tf.maximum(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "            x: np.random.randn(N, D),\n",
    "            w1: np.random.randn(D, H),\n",
    "            w2: np.random.randn(H, D),\n",
    "            y: np.random.randn(N, D),}\n",
    "    out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)\n",
    "    loss_val, grad_w1_val, grad_w2_val = out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 20. 이상\n",
    "import tensorflow as tf \n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "w1 = tf.Variable(tf.random.uniform((D, H)))\n",
    "w2 = tf.Variable(tf.random.uniform((H, D)))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "    y_pred = tf.matmul(h, w2)\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "gradients = tape.gradient(loss, [w1,w2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 20. 이상\n",
    "import tensorflow as tf \n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "w1 = tf.Variable(tf.random.uniform((D, H)))\n",
    "w2 = tf.Variable(tf.random.uniform((H, D)))\n",
    "\n",
    "optimizer = tf.optimizers.SGD(1e-6)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "        y_pred = tf.matmul(h, w2)\n",
    "        diff = y_pred - y\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "    gradients = tape.gradient(loss, [w1,w2])\n",
    "    optimizer.apply_gradients(zip(gradients, [w1, w2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 loss\n",
    "import tensorflow as tf \n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "w1 = tf.Variable(tf.random.uniform((D, H)))\n",
    "w2 = tf.Variable(tf.random.uniform((H, D)))\n",
    "\n",
    "optimizer = tf.optimizers.SGD(1e-6)\n",
    "\n",
    "for t in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "        y_pred = tf.matmul(h, w2)\n",
    "        diff = y_pred - y\n",
    "        loss = tf.losses.MeanSquaredError()(y_pred,y)\n",
    "    gradients = tape.gradient(loss, [w1,w2])\n",
    "    optimizer.apply_gradients(zip(gradients, [w1, w2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kearas : 사용\n",
    "import tensorflow as tf \n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D,), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(D))\n",
    "\n",
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "\n",
    "losses = []\n",
    "for t in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "        loss = tf.losses.MeanSquaredError()(y_pred,y)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kearas : 사용\n",
    "import tensorflow as tf \n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D,), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(D))\n",
    "\n",
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "\n",
    "model.complie(loss=tf.keras.losses.MeanSquaredError(),optimizer=optimizer)\n",
    "\n",
    "history = model.fit(x, y, epochs= 50, batch_size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Kearas : 사용\n",
    "import tensorflow as tf \n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D,), activation=tf.nn.relu))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(D))\n",
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "\n",
    "@tf.function\n",
    "def model_func(x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = tf.losses.MeanSquaredError()(y_pred, y)\n",
    "    return y_pred, loss\n",
    "for t in range(50):\n",
    "    with tf.GradienTape() as tape:\n",
    "        y_pred, loss = model_func(x, y)\n",
    "    gradients = tape.gradient(\n",
    "        loss, model.trainable_variables\n",
    "    )\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "import tensorflow as tf \n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D,), activation=tf.nn.relu))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(D))\n",
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "\n",
    "@tf.function\n",
    "def model_static(x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = tf.losses.MeanSquaredError()(y_pred, y)\n",
    "    return y_pred, loss\n",
    "\n",
    "def model_dynamic(x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = tf.losses.MeanSquaredError()(y_pred, y)\n",
    "    return y_pred, loss\n",
    "\n",
    "print(\"static graph:\", timeit.timeit(lambda: model_static(x, y), number= 10))\n",
    "print(\"dynamic graph:\", timeit.timeit(lambda: model_dynamic(x, y), number= 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}